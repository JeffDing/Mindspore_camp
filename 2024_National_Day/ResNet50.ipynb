{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530389db",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **ResNet50中药炮制饮片质量判断**\n",
    "### **背景介绍**\n",
    "中药炮制是根据中医药理论，依照临床辨证施治用药的需要和药物自身性质，以及调剂、制剂的不同要求，**将中药材制备成中药饮片所采取的一项制药技术**。平时我们老百姓能接触到的中药，主要指自己回家煎煮或者请医院代煎煮的中药，都是中药饮片，也就是中药炮制这个技术的结果。而中药炮制饮片，大部分涉及到水火的处理，一定需要讲究“程度适中”，炮制火候不够达不到最好药效，炮制火候过度也会丧失药效。\n",
    "- “生品”一般是指仅仅采用简单净选得到的饮片，通常没有经过火的处理，也是后续用火加工的原料。\n",
    "- “不及”就是“炮制不到位”，没有达到规定的程度，饮片不能发挥最好的效果。\n",
    "- “适中”是指炮制程度刚刚好，正是一个最佳的炮制点位，也是通常炮制结束的终点。\n",
    "- “太过”是指炮制程度过度了，超过了“适中”的最佳状态，这时候的饮片也会丧失药效，不能再使用了。\n",
    "\n",
    "过去的炮制饮片程度的判断，都是采用的老药工经验判断，但随着老药工人数越来越少，这种经验判断可能存在“失传”的风险。而随着人工智能的发展，使用深度神经网络模型对饮片状态进行判断能达到很好的效果，可以很好的实现经验的“智能化”和经验的传承。\n",
    "\n",
    "#### **ResNet网络简介**\n",
    "ResNet50网络是2015年由微软实验室的何恺明提出，获得ILSVRC2015图像分类竞赛第一名。在ResNet网络提出之前，传统的卷积神经网络都是将一系列的卷积层和池化层堆叠得到的，但当网络堆叠到一定深度时，就会出现退化问题。下图是在CIFAR-10数据集上使用56层网络与20层网络训练误差和测试误差图，由图中数据可以看出，56层网络比20层网络训练误差和测试误差更大，随着网络的加深，其误差并没有如预想的一样减小。<br>\n",
    "<img src=\"./images/image-20230804110559.png\" alt=\"image-20230630102435999\" style=\"zoom:30%;\" width=\"60%\" />\n",
    "\n",
    "ResNet网络提出了残差网络结构(Residual Network)来减轻退化问题，使用ResNet网络可以实现搭建较深的网络结构（突破1000层）。论文中使用ResNet网络在CIFAR-10数据集上的训练误差与测试误差图如下图所示，图中虚线表示训练误差，实线表示测试误差。由图中数据可以看出，ResNet网络层数越深，其训练误差和测试误差越小。<br>\n",
    "<img src=\"./images/image-20230804110625.png\" alt=\"image-20230630102435999\" style=\"zoom:30%;\" width=\"40%\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68406d8c-b9df-449e-9c12-bb89143c9c46",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **准备阶段**\n",
    "#### **配置实验环境**\n",
    "本案例支持在Ascend平台运行：\n",
    "- 平台：华为启智平台调试任务\n",
    "- 框架：MindSpore1.8.1/MindSpore2.3.0\n",
    "- 硬件NPU：Ascend 910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9cc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 实验环境已经预装了mindspore==2.3.0，如需更换mindspore版本，可更改下面 MINDSPORE_VERSION 变量\n",
    "!pip uninstall mindspore -y\n",
    "%env MINDSPORE_VERSION=2.3.0\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/${MINDSPORE_VERSION}/MindSpore/unified/aarch64/mindspore-${MINDSPORE_VERSION}-cp39-cp39-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.mirrors.ustc.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看当前 mindspore 版本\n",
    "!pip show mindspore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0a92b-0e0b-4b99-9748-fbaea042fd5f",
   "metadata": {},
   "source": [
    "#### **数据集介绍**\n",
    "\n",
    "我们使用“中药炮制饮片”数据集，该数据集由成都中医药大学提供，共包含中药炮制饮片的 3 个品种，分别为：蒲黄、山楂、王不留行，每个品种又有着4种炮制状态：生品、不及适中、太过，每类包含 500 张图片共12类5000张图片，图片尺寸为 4K，图片格式为 jpg。\n",
    "下面是数据集中的一些样例图片：\n",
    "\n",
    "<img src=\"./images/image-20230508160428881.png\" alt=\"image-20230630102435999\" style=\"zoom:67%;\" width=\"90%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707edc02-9007-45de-8a50-1c02672a6506",
   "metadata": {},
   "source": [
    "#### **准备数据**\n",
    "我们使用的数据集在调试任务中配置加载，并修改代码中对应url链接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60d946-63e4-4364-82f6-cfaca6e17ad3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from download import download\n",
    "import os\n",
    "url = \"https://mindspore-courses.obs.cn-north-4.myhuaweicloud.com/deep%20learning/AI%2BX/data/zhongyiyao.zip\"\n",
    "# 创建的是调试任务，url修改为数据集上传生成的url链接\n",
    "if not os.path.exists(\"dataset\"):\n",
    "    download(url, \"dataset\", kind=\"zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd962595-1e6d-4b85-a79a-2a356788de42",
   "metadata": {},
   "source": [
    "#### **数据预处理**\n",
    "原图片尺寸为4k比较大，我们预处理将图片resize到指定尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21808c-ef23-46e0-8857-d54dceed459a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "data_dir = \"dataset/zhongyiyao/zhongyiyao\"\n",
    "new_data_path = \"dataset1/zhongyiyao\"\n",
    "if not os.path.exists(new_data_path):\n",
    "    for path in ['train','test']:\n",
    "        data_path = data_dir + \"/\" + path\n",
    "        classes = os.listdir(data_path)\n",
    "        for (i,class_name) in enumerate(classes):\n",
    "            floder_path =  data_path+\"/\"+class_name\n",
    "            print(f\"正在处理{floder_path}...\")\n",
    "            for image_name in os.listdir(floder_path):\n",
    "                try:\n",
    "                    image = Image.open(floder_path + \"/\" + image_name)\n",
    "                    image = image.resize((1000,1000))\n",
    "                    target_dir = new_data_path+\"/\"+path+\"/\"+class_name\n",
    "                    if not os.path.exists(target_dir):\n",
    "                        os.makedirs(target_dir)\n",
    "                    if not os.path.exists(target_dir+\"/\"+image_name):\n",
    "                        image.save(target_dir+\"/\"+image_name)\n",
    "                except:\n",
    "                    pass     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c62ea-8425-406c-baed-3d2c3fe17ca2",
   "metadata": {},
   "source": [
    "#### **数据集划分**\n",
    "数据集已划分为训练集和测试集，可以接着继续划分验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099471e-3942-4d87-b615-33629ef9b218",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    This function splits the data into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size/(1-test_size), random_state=random_state)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "data_dir = \"dataset1/zhongyiyao\"\n",
    "floders = os.listdir(data_dir)\n",
    "target = ['train','test','valid']\n",
    "if set(floders) == set(target):\n",
    "    # 如果已经划分则跳过\n",
    "    pass\n",
    "elif 'train' in floders:\n",
    "    # 如果已经划分了train，test，那么只需要从train里边划分出valid\n",
    "    floders = os.listdir(data_dir)\n",
    "    new_data_dir = os.path.join(data_dir,'train')\n",
    "    classes = os.listdir(new_data_dir)\n",
    "    if '.ipynb_checkpoints' in classes:\n",
    "        classes.remove('.ipynb_checkpoints')\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for (i,class_name) in enumerate(classes):\n",
    "        new_path =  new_data_dir+\"/\"+class_name\n",
    "        for image_name in os.listdir(new_path):\n",
    "            imgs.append(image_name)\n",
    "            labels.append(class_name)\n",
    "    imgs_train,imgs_val,labels_train,labels_val = X_train, X_test, y_train, y_test = train_test_split(imgs, labels, test_size=0.2, random_state=42)\n",
    "    print(\"划分训练集图片数：\",len(imgs_train))\n",
    "    print(\"划分验证集图片数：\",len(imgs_val))\n",
    "    target_data_dir = os.path.join(data_dir,'valid')\n",
    "    if not os.path.exists(target_data_dir):\n",
    "        os.mkdir(target_data_dir)\n",
    "    for (img,label) in zip(imgs_val,labels_val):\n",
    "        source_path = os.path.join(data_dir,'train',label)\n",
    "        target_path = os.path.join(data_dir,'valid',label)\n",
    "        if not os.path.exists(target_path):\n",
    "            os.mkdir(target_path)\n",
    "        source_img = os.path.join(source_path,img)\n",
    "        target_img = os.path.join(target_path,img)\n",
    "        shutil.move(source_img,target_img)\n",
    "else:\n",
    "    phones = os.listdir(data_dir)\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for phone in phones:\n",
    "        phone_data_dir = os.path.join(data_dir,phone)\n",
    "        yaowu_list = os.listdir(phone_data_dir)\n",
    "        for yaowu in yaowu_list:\n",
    "            yaowu_data_dir = os.path.join(phone_data_dir,yaowu)\n",
    "            chengdu_list = os.listdir(yaowu_data_dir)\n",
    "            for chengdu in chengdu_list:\n",
    "                chengdu_data_dir = os.path.join(yaowu_data_dir,chengdu)\n",
    "                for img in os.listdir(chengdu_data_dir):\n",
    "                    imgs.append(img)\n",
    "                    label = ' '.join([phone,yaowu,chengdu])\n",
    "                    labels.append(label)\n",
    "    imgs_train, imgs_val, imgs_test, labels_train, labels_val, labels_test = split_data(imgs, labels, test_size=0.2, val_size=0.2, random_state=42)\n",
    "    img_label_tuple_list = [(imgs_train,labels_train),(imgs_val,labels_val),(imgs_test,labels_test)]\n",
    "    for (i,split) in enumerate(spilits):\n",
    "        target_data_dir = os.path.join(data_dir,split)\n",
    "        if not os.path.exists(target_data_dir):\n",
    "            os.mkdir(target_data_dir)\n",
    "        imgs_list,labels_list = img_label_tuple_list[i]\n",
    "        for (img,label) in zip(imgs_list,labels_list):\n",
    "            label_split = label.split(' ')\n",
    "            source_img = os.path.join(data_dir,label_split[0],label_split[1],label_split[2],img)\n",
    "            target_img_dir = os.path.join(target_data_dir,label_split[1]+\"_\"+label_split[2])\n",
    "            if not os.path.exists(target_img_dir):\n",
    "                os.mkdir(target_img_dir)\n",
    "            target_img = os.path.join(target_img_dir,img)\n",
    "            shutil.move(source_img,target_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f575be-4b4d-43d6-8225-42286c9de59b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **定义数据加载方式**\n",
    "通过重写Iterable类的方式加载图片数据集，支持单张图片和文件夹进行加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7fbc3-6f39-4d37-be0a-ace0ec052ecb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mindspore.dataset import GeneratorDataset\n",
    "import mindspore.dataset.vision as vision\n",
    "import mindspore.dataset.transforms as transforms\n",
    "from mindspore import dtype as mstype\n",
    "# 注意没有使用Mindspore提供的ImageFloder进行加载，原因是调试任务中'.ipynb_checkpoints'缓存文件夹会被当作类文件夹进行识别，导致数据集加载错误\n",
    "class Iterable:\n",
    "    def __init__(self,data_path):\n",
    "        self._data = []\n",
    "        self._label = []\n",
    "        self._error_list = []\n",
    "        if data_path.endswith(('JPG','jpg','png','PNG')):\n",
    "            # 用作推理，所以没有label\n",
    "            image = Image.open(data_path)\n",
    "            self._data.append(image)\n",
    "            self._label.append(0)\n",
    "        else:\n",
    "            classes = os.listdir(data_path)\n",
    "            if '.ipynb_checkpoints' in classes:\n",
    "                classes.remove('.ipynb_checkpoints')\n",
    "            for (i,class_name) in enumerate(classes):\n",
    "                new_path =  data_path+\"/\"+class_name\n",
    "                for image_name in os.listdir(new_path):\n",
    "                    try:\n",
    "                        image = Image.open(new_path + \"/\" + image_name)\n",
    "                        self._data.append(image)\n",
    "                        self._label.append(i)\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._data[index], self._label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def get_error_list(self,):\n",
    "        return self._error_list\n",
    "    \n",
    "def create_dataset_zhongyao(dataset_dir,usage,resize,batch_size,workers):\n",
    "    data = Iterable(dataset_dir)\n",
    "    data_set = GeneratorDataset(data,column_names=['image','label'])\n",
    "    trans = []\n",
    "    if usage == \"train\":\n",
    "        trans += [\n",
    "            vision.RandomCrop(700, (4, 4, 4, 4)),\n",
    "            # 这里随机裁剪尺度可以设置\n",
    "            vision.RandomHorizontalFlip(prob=0.5)\n",
    "        ]\n",
    "\n",
    "    trans += [\n",
    "        vision.Resize((resize,resize)),\n",
    "        vision.Rescale(1.0 / 255.0, 0.0),\n",
    "        vision.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "    target_trans = transforms.TypeCast(mstype.int32)\n",
    "    # 数据映射操作\n",
    "    data_set = data_set.map(\n",
    "        operations=trans,\n",
    "        input_columns='image',\n",
    "        num_parallel_workers=workers)\n",
    "\n",
    "    data_set = data_set.map(\n",
    "        operations=target_trans,\n",
    "        input_columns='label',\n",
    "        num_parallel_workers=workers)\n",
    "\n",
    "    # 批量操作\n",
    "    data_set = data_set.batch(batch_size,drop_remainder=True)\n",
    "\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c148c-3fde-4454-a010-63c542adf1b0",
   "metadata": {},
   "source": [
    "#### **加载数据**\n",
    "对数据集使用定义好的方式进行加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fff09-ad22-42b4-a2ef-6be8ea8f2192",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "import random\n",
    "data_dir = \"dataset1/zhongyiyao\"\n",
    "train_dir = data_dir+\"/\"+\"train\"\n",
    "valid_dir = data_dir+\"/\"+\"valid\"\n",
    "test_dir = data_dir+\"/\"+\"test\"\n",
    "batch_size = 32 # 批量大小\n",
    "image_size = 224 # 训练图像空间大小\n",
    "workers = 4 # 并行线程个数\n",
    "num_classes = 12 # 分类数量\n",
    "\n",
    "\n",
    "# 设置随机种子，使得模型结果复现\n",
    "seed = 42\n",
    "ms.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "dataset_train = create_dataset_zhongyao(dataset_dir=train_dir,\n",
    "                                       usage=\"train\",\n",
    "                                       resize=image_size,\n",
    "                                       batch_size=batch_size,\n",
    "                                       workers=workers)\n",
    "step_size_train = dataset_train.get_dataset_size()\n",
    "\n",
    "dataset_val = create_dataset_zhongyao(dataset_dir=valid_dir,\n",
    "                                     usage=\"valid\",\n",
    "                                     resize=image_size,\n",
    "                                     batch_size=batch_size,\n",
    "                                     workers=workers)\n",
    "dataset_test = create_dataset_zhongyao(dataset_dir=test_dir,\n",
    "                                     usage=\"test\",\n",
    "                                     resize=image_size,\n",
    "                                     batch_size=batch_size,\n",
    "                                     workers=workers)\n",
    "step_size_val = dataset_val.get_dataset_size()\n",
    "\n",
    "print(f'训练集数据：{dataset_train.get_dataset_size()*batch_size}\\n')\n",
    "print(f'验证集数据：{dataset_val.get_dataset_size()*batch_size}\\n')\n",
    "print(f'测试集数据：{dataset_test.get_dataset_size()*batch_size}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879aa417-cf54-4a49-acd5-8334d6121319",
   "metadata": {},
   "source": [
    "#### **类别标签说明**\n",
    "由于平台字体问题，无法正确显示中文，这里给出英文标签对应的类别：\n",
    "- ph-sp：蒲黄-生品\n",
    "- ph_bj：蒲黄-不及\n",
    "- ph_sz：蒲黄-适中\n",
    "- ph_tg：蒲黄-太过\n",
    "- sz_sp：山楂-生品\n",
    "- sz_bj：山楂-不及\n",
    "- sz_sz：山楂-适中\n",
    "- sz_tg：山楂-太过\n",
    "- wblx_sp：王不留行-生品\n",
    "- wblx_bj：王不留行-不及\n",
    "- wblx_sz：王不留行-适中\n",
    "- wblx_tg：王不留行-太过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d0204-ca87-4037-b454-a6266dd42b04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#index_label的映射\n",
    "index_label_dict = {}\n",
    "classes = os.listdir(train_dir)\n",
    "if '.ipynb_checkpoints' in classes:\n",
    "    classes.remove('.ipynb_checkpoints')\n",
    "for i,label in enumerate(classes):\n",
    "    index_label_dict[i] = label\n",
    "label2chin = {'ph_sp':'蒲黄-生品',  'ph_bj':'蒲黄-不及', 'ph_sz':'蒲黄-适中', 'ph_tg':'蒲黄-太过', 'sz_sp':'山楂-生品',\n",
    "              'sz_bj':'山楂-不及', 'sz_sz':'山楂-适中', 'sz_tg':'山楂-太过', 'wblx_sp':'王不留行-生品', 'wblx_bj':'王不留行-不及',\n",
    "              'wblx_sz':'王不留行-适中', 'wblx_tg':'王不留行-太过'}\n",
    "index_label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095c67f-4268-4a7a-b5c2-7ea3aa436a79",
   "metadata": {},
   "source": [
    "#### **数据可视化展示**\n",
    "对已经加载好的数据集，选取部分数据展示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da162613-0ac8-46ee-94c3-2065adb5b91f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_iter = next(dataset_val.create_dict_iterator())\n",
    "\n",
    "images = data_iter[\"image\"].asnumpy()\n",
    "labels = data_iter[\"label\"].asnumpy()\n",
    "print(f\"Image shape: {images.shape}, Label: {labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5379b-a945-4e5e-b4be-d890b0474941",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(24):\n",
    "    plt.subplot(3, 8, i+1)\n",
    "    image_trans = np.transpose(images[i], (1, 2, 0))\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])\n",
    "    image_trans = std * image_trans + mean\n",
    "    image_trans = np.clip(image_trans, 0, 1)\n",
    "    plt.title(index_label_dict[labels[i]])\n",
    "    plt.imshow(image_trans)\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce262fb-8137-408d-bd39-b8d1627730d8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **创建分类网络**\n",
    "当处理完数据后，就可以来进行网络的搭建了。我们使用经典的Resnet50作为基础模型，模型由MindSpore框架编写，我们在此基础上进行了修改，将最后一层的输出改为类别数12，以适应我们的数据集。\n",
    "\n",
    "#### **模型结构展示**\n",
    "\n",
    "<img src=\"./images/image-20230630095910087.png\" alt=\"image-20230630095910087\" style=\"zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fec773-7fc8-4f5a-9038-2e878d843b5f",
   "metadata": {},
   "source": [
    "#### **构建残差网络结构**\n",
    "残差结构是ResNet网络中最重要的结构，其结构图如下图所示，残差网络由两个分支构成：一个主分支，一个shortcuts（图中弧线表示）。主分支通过堆叠一系列的卷积操作得到，shortcuts从输入直接到输出，主分支的输出与shortcuts的输出相加后通过Relu激活函数后即为残差网络最后的输出。<br>\n",
    "<img src=\"./images/image-20230804113155.png\" alt=\"image-20230630102435999\" style=\"zoom:67%;\" width=\"40%\"/>\n",
    "\n",
    "残差网络结构主要由两种，一种是Building Block，适用于较浅的ResNet网络，如ResNet18和ResNet34；另一种是Bottleneck，适用于层数较深的ResNet网络，如ResNet50、ResNet101和ResNet152。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f4dac-4080-403b-9097-496c5378fca9",
   "metadata": {},
   "source": [
    "#### **定义 Building Block**\n",
    "Building Block结构图如下图所示，主分支有两层卷积网络结构：\n",
    "\n",
    "- 主分支第一层网络以输入channel为64为例，首先通过一个3×3的卷积层，然后通过Batch Normalization层，最后通过Relu激活函数层，输出channel为64；\n",
    "\n",
    "- 主分支第二层网络的输入channel为64，首先通过一个3×3的卷积层，然后通过Batch Normalization层，输出channel为64。\n",
    "\n",
    "最后将主分支输出的特征矩阵与shortcuts输出的特征矩阵相加，通过Relu激活函数即为Building Block最后的输出。<br>\n",
    "<img src=\"./images/image-20230804114627.png\" alt=\"image-20230630102435999\" style=\"zoom:67%;\" width=\"30%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7244808-25d2-4138-a0e6-7df216215230",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mindspore import Model\n",
    "from mindspore import context\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor, nn, set_context, GRAPH_MODE, train\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "from typing import Type, Union, List, Optional\n",
    "from mindspore import nn, train\n",
    "from mindspore.common.initializer import Normal\n",
    "\n",
    "\n",
    "weight_init = Normal(mean=0, sigma=0.02)\n",
    "gamma_init = Normal(mean=1, sigma=0.02)\n",
    "\n",
    "class ResidualBlockBase(nn.Cell):\n",
    "    expansion: int = 1  # 最后一个卷积核数量与第一个卷积核数量相等\n",
    "\n",
    "    def __init__(self, in_channel: int, out_channel: int,\n",
    "                 stride: int = 1, norm: Optional[nn.Cell] = None,\n",
    "                 down_sample: Optional[nn.Cell] = None) -> None:\n",
    "        super(ResidualBlockBase, self).__init__()\n",
    "        if not norm:\n",
    "            self.norm = nn.BatchNorm2d(out_channel)\n",
    "        else:\n",
    "            self.norm = norm\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel,\n",
    "                               kernel_size=3, stride=stride,\n",
    "                               weight_init=weight_init)\n",
    "        self.conv2 = nn.Conv2d(in_channel, out_channel,\n",
    "                               kernel_size=3, weight_init=weight_init)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down_sample = down_sample\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"ResidualBlockBase construct.\"\"\"\n",
    "        identity = x  # shortcuts分支\n",
    "\n",
    "        out = self.conv1(x)  # 主分支第一层：3*3卷积层\n",
    "        out = self.norm(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)  # 主分支第二层：3*3卷积层\n",
    "        out = self.norm(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(x)\n",
    "        out += identity  # 输出为主分支与shortcuts之和\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f32ba-ce1c-4c6d-83b4-1b799da62621",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#### **定义 Bottleneck**\n",
    "Bottleneck结构图如下图所示，在输入相同的情况下Bottleneck结构相对Building Block结构的参数数量更少，更适合层数较深的网络，ResNet50使用的残差结构就是Bottleneck。该结构的主分支有三层卷积结构，分别为1×1的卷积层、3×3卷积层和\n",
    "1×1的卷积层，其中1×1的卷积层分别起降维和升维的作用。\n",
    "\n",
    "- 主分支第一层网络以输入channel为256为例，首先通过数量为64，大小为的卷积核进行降维，然后通过Batch Normalization层，最后通过Relu激活函数层，其输出channel为64；\n",
    "\n",
    "- 主分支第二层网络通过数量为64，大小为的卷积核提取特征，然后通过Batch Normalization层，最后通过Relu激活函数层，其输出channel为64；\n",
    "\n",
    "- 主分支第三层通过数量为256，大小的卷积核进行升维，然后通过Batch Normalization层，其输出channel为256。\n",
    "\n",
    "最后将主分支输出的特征矩阵与shortcuts输出的特征矩阵相加，通过Relu激活函数即为Bottleneck最后的输出。<br>\n",
    "<img src=\"./images/image-20230804115129.png\" alt=\"image-20230630102435999\" style=\"zoom:67%;\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a0809-685f-4981-87f3-1fcf01cb5ad2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Cell):\n",
    "    expansion = 4  # 最后一个卷积核的数量是第一个卷积核数量的4倍\n",
    "\n",
    "    def __init__(self, in_channel: int, out_channel: int,\n",
    "                 stride: int = 1, down_sample: Optional[nn.Cell] = None) -> None:\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel,\n",
    "                               kernel_size=1, weight_init=weight_init)\n",
    "        self.norm1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel,\n",
    "                               kernel_size=3, stride=stride,\n",
    "                               weight_init=weight_init)\n",
    "        self.norm2 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv3 = nn.Conv2d(out_channel, out_channel * self.expansion,\n",
    "                               kernel_size=1, weight_init=weight_init)\n",
    "        self.norm3 = nn.BatchNorm2d(out_channel * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.down_sample = down_sample\n",
    "\n",
    "    def construct(self, x):\n",
    "\n",
    "        identity = x  # shortscuts分支\n",
    "\n",
    "        out = self.conv1(x)  # 主分支第一层：1*1卷积层\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)  # 主分支第二层：3*3卷积层\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)  # 主分支第三层：1*1卷积层\n",
    "        out = self.norm3(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            identity = self.down_sample(x)\n",
    "\n",
    "        out += identity  # 输出为主分支与shortcuts之和\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed54738c-c2c6-480e-9315-450e1100c727",
   "metadata": {},
   "source": [
    "#### **构建ResNet网络**\n",
    "ResNet网络由基本的Building Block/Bottleneck块堆叠而成，我们将重复的结构定义为残差网络块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59084fe2-94f0-4dc1-82d1-34df5643c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layer(last_out_channel, block: Type[Union[ResidualBlockBase, ResidualBlock]],\n",
    "               channel: int, block_nums: int, stride: int = 1):\n",
    "    down_sample = None  # shortcuts分支\n",
    "\n",
    "\n",
    "    if stride != 1 or last_out_channel != channel * block.expansion:\n",
    "\n",
    "        down_sample = nn.SequentialCell([\n",
    "            nn.Conv2d(last_out_channel, channel * block.expansion,\n",
    "                      kernel_size=1, stride=stride, weight_init=weight_init),\n",
    "            nn.BatchNorm2d(channel * block.expansion, gamma_init=gamma_init)\n",
    "        ])\n",
    "\n",
    "    layers = []\n",
    "    layers.append(block(last_out_channel, channel, stride=stride, down_sample=down_sample))\n",
    "\n",
    "    in_channel = channel * block.expansion\n",
    "    # 堆叠残差网络\n",
    "    for _ in range(1, block_nums):\n",
    "\n",
    "        layers.append(block(in_channel, channel))\n",
    "\n",
    "    return nn.SequentialCell(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d44f2c-1e86-48d8-95a2-ee97b29de56c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "from mindspore import ops\n",
    "\n",
    "\n",
    "class ResNet(nn.Cell):\n",
    "    def __init__(self, block: Type[Union[ResidualBlockBase, ResidualBlock]],\n",
    "                 layer_nums: List[int], num_classes: int, input_channel: int) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        # 第一个卷积层，输入channel为3（彩色图像），输出channel为64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, weight_init=weight_init)\n",
    "        self.norm = nn.BatchNorm2d(64)\n",
    "        # 最大池化层，缩小图片的尺寸\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode='same')\n",
    "        # 各个残差网络结构块定义\n",
    "        self.layer1 = make_layer(64, block, 64, layer_nums[0])\n",
    "        self.layer2 = make_layer(64 * block.expansion, block, 128, layer_nums[1], stride=2)\n",
    "        self.layer3 = make_layer(128 * block.expansion, block, 256, layer_nums[2], stride=2)\n",
    "        self.layer4 = make_layer(256 * block.expansion, block, 512, layer_nums[3], stride=2)\n",
    "        # 平均池化层\n",
    "        self.avg_pool = ops.ReduceMean(keep_dims=True)\n",
    "        # self.avg_pool = nn.AvgPool2d()\n",
    "        \n",
    "        # flattern层\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 全连接层\n",
    "        self.fc = nn.Dense(in_channels=input_channel, out_channels=num_classes)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avg_pool(x,(2,3))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd573d-8c14-40dc-a394-e472dfef1ca5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _resnet(model_url: str, block: Type[Union[ResidualBlockBase, ResidualBlock]],\n",
    "            layers: List[int], num_classes: int, pretrained: bool, pretrained_ckpt: str,\n",
    "            input_channel: int):\n",
    "    model = ResNet(block, layers, num_classes, input_channel)\n",
    "\n",
    "    if pretrained:\n",
    "        # 加载预训练模型\n",
    "        download(url=model_url, path=pretrained_ckpt)\n",
    "        param_dict = load_checkpoint(pretrained_ckpt)\n",
    "        load_param_into_net(model, param_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(num_classes: int = 1000, pretrained: bool = False):\n",
    "    resnet50_url = \"https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/models/application/resnet50_224_new.ckpt\"\n",
    "    resnet50_ckpt = \"./LoadPretrainedModel/resnet50_224_new.ckpt\"\n",
    "    return _resnet(resnet50_url, ResidualBlock, [3, 4, 6, 3], num_classes,\n",
    "                   pretrained, resnet50_ckpt, 2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044dad8-56ad-44de-81d6-18f11e495dd8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### **ResNet分类模型初始化**\n",
    "模型定义完成后，实例化ResNet分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c2db4-e68a-4172-a2af-3125d21cd229",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "network = resnet50(pretrained=True)\n",
    "num_class = 12\n",
    "# 全连接层输入层的大小\n",
    "in_channel = network.fc.in_channels\n",
    "fc = nn.Dense(in_channels=in_channel, out_channels=num_class)\n",
    "# 重置全连接层\n",
    "network.fc = fc\n",
    "\n",
    "for param in network.get_parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dda3cc-8a5c-4571-88eb-7b5221a83957",
   "metadata": {},
   "source": [
    "### **模型训练**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90966497-6287-4c59-9dec-fd27ac73f16c",
   "metadata": {},
   "source": [
    "#### **定义训练参数**\n",
    "\n",
    "我们设置epoch为50,Momentum作为优化器，其中参数momentum设为0.9，而损失函数则采用SoftmaxCrossEntropyWithLogits。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b24b5-4ec5-41a7-b4e6-d852f5b49066",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "# early stopping\n",
    "patience = 5\n",
    "lr = nn.cosine_decay_lr(min_lr=0.00001, max_lr=0.001, total_step=step_size_train * num_epochs,\n",
    "                        step_per_epoch=step_size_train, decay_epoch=num_epochs)\n",
    "opt = nn.Momentum(params=network.trainable_params(), learning_rate=lr, momentum=0.9)\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "model = ms.Model(network, loss_fn, opt, metrics={'acc'})\n",
    "\n",
    "# 最佳模型存储路径\n",
    "best_acc = 0\n",
    "best_ckpt_dir = \"./BestCheckpoint\"\n",
    "best_ckpt_path = \"./BestCheckpoint/resnet50-best.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4356845-ee65-4cf6-bb91-9abe3b2db088",
   "metadata": {},
   "source": [
    "#### **定义训练推理函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d239ed5-536c-4e63-b391-61d9a6dd131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, dataset, loss_fn, optimizer):\n",
    "    # Define forward function\n",
    "    def forward_fn(data, label):\n",
    "        logits = model(data)\n",
    "        loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "\n",
    "    # Get gradient function\n",
    "    grad_fn = ms.ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "    # Define function of one-step training\n",
    "    def train_step(data, label):\n",
    "        (loss, _), grads = grad_fn(data, label)\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "\n",
    "        if batch % 100 == 0 or batch == step_size_train - 1:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def test_loop(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        y_true.extend(label.asnumpy().tolist())\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        y_pred.extend(pred.argmax(1).asnumpy().tolist())\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    print(classification_report(y_true,y_pred,target_names= list(index_label_dict.values()),digits=3))\n",
    "    return correct,test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa7a87-f9df-4050-9af0-a6cfccca4543",
   "metadata": {},
   "source": [
    "#### **开始训练**\n",
    "\n",
    "在每个训练轮次中，使用训练集进行模型训练，并计算交叉熵损失以更新参数。随后在验证集上对模型进行测试，并以\"accuracy\"作为评价指标来评估模型的性能。为了防止过拟合，我们引入了早停机制，通过监控验证集上的指标，及时停止训练以避免过拟合，并保存具有最佳性能的模型参数。通过这样的训练过程，我们期望模型能够逐渐优化，并达到更好的性能水平。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0469532-ce10-410a-80c2-4553c9c6b7e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_improvement_count = 0\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "stop_epoch = num_epochs\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(network, dataset_train, loss_fn, opt)\n",
    "    acc,loss = test_loop(network, dataset_val, loss_fn)\n",
    "    acc_list.append(acc)\n",
    "    loss_list.append(loss)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        if not os.path.exists(best_ckpt_dir):\n",
    "            os.mkdir(best_ckpt_dir)\n",
    "        ms.save_checkpoint(network, best_ckpt_path)\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "        if no_improvement_count > patience:\n",
    "            print('Early stopping triggered. Restoring best weights...')\n",
    "            stop_epoch = t\n",
    "            break \n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e1ade-daee-45e7-a74d-d79e0952d2d6",
   "metadata": {},
   "source": [
    "#### **结果可视化展示**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeaaaaa-dd5a-475b-ad47-d7664e9f2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_process(acc_list, loss_list):\n",
    "    epochs = range(1, len(acc_list) + 1)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    # 绘制准确率曲线\n",
    "    plt.subplot(121)\n",
    "    plt.plot(epochs, acc_list, 'b-', label='Training Accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # 绘制损失函数曲线\n",
    "    plt.subplot(122)\n",
    "    plt.plot(epochs, loss_list, 'r-', label='Training Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # 调整子图间距\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "    # 显示图像\n",
    "    plt.show()\n",
    "\n",
    "# 调用函数绘制训练过程图\n",
    "plot_training_process(acc_list, loss_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a506e4-9892-4f43-a377-7fcaf177a821",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **模型推理**\n",
    "在模型推理阶段，我们提供了两种预测推理方式：单张图片推理和数据集推理方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25cf05c-0ef3-4c94-993f-695811d0520e",
   "metadata": {},
   "source": [
    "#### **加载模型**\n",
    "首先加载训练好了的最佳模型权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630601bc-08f8-4727-9992-18e84139d376",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_class = 12  # \n",
    "net = resnet50(num_class)\n",
    "best_ckpt_path = 'BestCheckpoint/resnet50-best.ckpt'\n",
    "# 加载模型参数\n",
    "param_dict = ms.load_checkpoint(best_ckpt_path)\n",
    "ms.load_param_into_net(net, param_dict)\n",
    "model = ms.Model(net)\n",
    "image_size = 224\n",
    "workers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ade577-6e1f-4fa8-9847-2b6cf4641a17",
   "metadata": {},
   "source": [
    "#### **通过传入图片路径进行推理**\n",
    "给定单张图片的路径推理预测分类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f384425-39c6-40e0-b6de-55ae9b2f099e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_one(input_img):\n",
    "    # 加载验证集的数据进行验证\n",
    "    dataset_one = create_dataset_zhongyao(dataset_dir=input_img,\n",
    "                                       usage=\"test\",\n",
    "                                       resize=image_size,\n",
    "                                       batch_size=1,\n",
    "                                       workers=workers)\n",
    "    data = next(dataset_one.create_tuple_iterator())\n",
    "    # print(data)\n",
    "    images = data[0].asnumpy()\n",
    "    labels = data[1].asnumpy()\n",
    "    # 预测图像类别\n",
    "    output = model.predict(ms.Tensor(data[0]))\n",
    "    pred = np.argmax(output.asnumpy(), axis=1)\n",
    "    # print(f'预测结果：{index_label_dict[pred[0]]}')\n",
    "    return index_label_dict[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73f9cf-9c7f-414e-90e7-8b09ad00b79a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_img = \"dataset1/zhongyiyao/train/sz_tg/IMG_6002.JPG\"\n",
    "print(predict_one(input_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5630d16-ea0f-4b93-a41a-5d796b6092f8",
   "metadata": {},
   "source": [
    "#### **通过传入测试数据集进行推理**\n",
    "直接给定测试数据集，模型将对数据集中的样本进行预测，并生成可视化展示来呈现推理结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c69cfe-8097-45f1-ac9a-df894b1091bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_model(dataset_test):\n",
    "    # 加载验证集的数据进行验证\n",
    "    data = next(dataset_test.create_tuple_iterator())\n",
    "    # print(data)\n",
    "    images = data[0].asnumpy()\n",
    "    labels = data[1].asnumpy()\n",
    "    # 预测图像类别\n",
    "    output = model.predict(ms.Tensor(data[0]))\n",
    "    pred = np.argmax(output.asnumpy(), axis=1)\n",
    "\n",
    "    # 显示图像及图像的预测值\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(6):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        color = 'blue' if pred[i] == labels[i] else 'red'\n",
    "        plt.title('predict:{}  actual:{}'.format(index_label_dict[pred[i]],index_label_dict[labels[i]]), color=color)\n",
    "        picture_show = np.transpose(images[i], (1, 2, 0))\n",
    "        mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "        std = np.array([0.2023, 0.1994, 0.2010])\n",
    "        picture_show = std * picture_show + mean\n",
    "        picture_show = np.clip(picture_show, 0, 1)\n",
    "        plt.imshow(picture_show)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78a050-89d8-4322-9d68-5da22dea6b8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualize_model(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0416a0-b6ce-44c7-b7d2-06b3319223cc",
   "metadata": {},
   "source": [
    "### **参考文献**\n",
    "[1] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3fbd24d2d81707c4b561437a4228ef79a00e041a3a9b4f7e2930dcc6bd46aa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
